{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '12'\n",
    "\n",
    "import torch\n",
    "import os.path as osp\n",
    "import GCL.losses as L\n",
    "import torch_geometric.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import GCL.augmentors as A\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch import nn, tensor\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from GCL.eval import get_split, LREvaluator, SVMEvaluator\n",
    "from GCL.models import SingleBranchContrast\n",
    "from torch_geometric.nn import GATConv, GCNConv, GATv2Conv\n",
    "from torch_geometric.nn.inits import uniform\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from scipy.io import loadmat\n",
    "from torch_geometric.data import Data\n",
    "from GCL.models.contrast_model import WithinEmbedContrast\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_mutual_info_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2):\n",
    "        super(GConv, self).__init__()\n",
    "        self.act = nn.SELU()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = nn.BatchNorm1d(hidden_dim, momentum=0.01)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            GATConv(input_dim, hidden_dim)\n",
    "        )\n",
    "        for _ in range(1, num_layers):\n",
    "            self.layers.append(\n",
    "                GATConv(hidden_dim, hidden_dim)\n",
    "            )\n",
    "\n",
    "    def forward(self, x, edge_index, edge_weight=None):\n",
    "        z = x\n",
    "        for i in range(self.num_layers - 1):\n",
    "            z = self.layers[i](z, edge_index, edge_weight)\n",
    "            z = self.act(z)\n",
    "        z = self.layers[-1](z, edge_index, edge_weight)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, encoder, augmentor, hidden_dim=256, n_clusters=3, v=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.augmentor = augmentor\n",
    "        self.register_buffer(\"epsilon\", torch.FloatTensor([1e-12]))\n",
    "\n",
    "        self.cluster_layer = nn.Parameter(torch.Tensor(n_clusters, hidden_dim))\n",
    "        self.v = v\n",
    "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "    def forward(self, x, edge_index_dict, edge_weight=None):\n",
    "        aug1, aug2 = self.augmentor\n",
    "        zs = []\n",
    "        z1s = []\n",
    "        z2s = []\n",
    "        for edge_index in edge_index_dict.values():\n",
    "            x1, edge_index1, edge_weight1 = aug1(x, edge_index, edge_weight)\n",
    "            # x2, edge_index2, edge_weight2 = aug2(x, edge_index, edge_weight)\n",
    "\n",
    "            z = self.encoder(x, edge_index, edge_weight)\n",
    "\n",
    "            z1 = self.encoder(x1, edge_index1, edge_weight1)\n",
    "            # z2 = self.encoder(x2, edge_index2, edge_weight2)\n",
    "\n",
    "            zs.append(z)\n",
    "            z1s.append(z1)\n",
    "            z2s.append(z)\n",
    "\n",
    "        # z = zs[0] + zs[1]\n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q = q.pow((self.v + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "        return z, z1s, z2s, q\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'PAP', 'PLP', 'PMP', 'PTP', 'feature', 'label', 'test_idx', 'train_idx', 'val_idx'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n",
    "\n",
    "def train(encoder_model, contrast_model, data, optimizer, p):\n",
    "    encoder_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    _, z1s, z2s, q = encoder_model(data.x, data.edge_index, data.edge_attr)\n",
    "    loss = None\n",
    "    for i in range(len(z1s)):\n",
    "        if loss is None:\n",
    "            loss = contrast_model(z1s[i], z2s[i])\n",
    "        else:\n",
    "            loss += contrast_model(z1s[i], z2s[i])\n",
    "    kl_loss = F.kl_div(q.log(), p, reduction='batchmean')\n",
    "    loss = 0.01 * loss + 100 * kl_loss\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), kl_loss.item()\n",
    "\n",
    "\n",
    "def test(encoder_model, data):\n",
    "    encoder_model.eval()\n",
    "    z, _, _, _ = encoder_model(data.x, data.edge_index, data.edge_attr)\n",
    "    split = get_split(num_samples=z.size()[0], train_ratio=0.1, test_ratio=0.8)\n",
    "    result = SVMEvaluator()(z, data.y, split)\n",
    "    return result\n",
    "\n",
    "\n",
    "device = torch.device('cuda')\n",
    "path = osp.join(osp.pardir, 'datasets', 'ACM')\n",
    "\n",
    "mat = loadmat(osp.join(path, 'ACM3025.mat'))\n",
    "print(mat.keys())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(mat['train_idx'][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [00:10<00:00, 290.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210761\n"
     ]
    }
   ],
   "source": [
    "# edge_index = []\n",
    "# for i in tqdm(range(len(mat['PLP']))):\n",
    "#     for j in range(len(mat['PLP'])):\n",
    "#         if mat['PAP'][i][j] == 1:\n",
    "#             edge_index.append([i, j])\n",
    "\n",
    "edge_index_dict = {}\n",
    "for etype in ['PLP']:\n",
    "    edge_index = []\n",
    "    for i in tqdm(range(len(mat[etype]))):\n",
    "        for j in range(len(mat[etype])):\n",
    "            if mat[etype][i][j] == 1:\n",
    "                edge_index.append([i, j])\n",
    "    print(len(edge_index))\n",
    "    edge_index = tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    edge_index_dict[etype] = edge_index"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "x = tensor(mat['feature'], dtype=torch.float)\n",
    "y = torch.argmax(tensor(mat['label']), -1)\n",
    "data = Data(x=x, y=y, edge_index=edge_index_dict).to(device)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "del mat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object Module.parameters at 0x000001B328B56E40>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "(T):   0%|          | 0/1000 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 8.00 GiB total capacity; 4.46 GiB already allocated; 0 bytes free; 6.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[1;32mIn [8]\u001B[0m, in \u001B[0;36m<cell line: 22>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     23\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m1\u001B[39m, epoch \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m):\n\u001B[0;32m     24\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m epoch \u001B[38;5;241m%\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m     25\u001B[0m         \u001B[38;5;66;03m# update_interval\u001B[39;00m\n\u001B[1;32m---> 26\u001B[0m         _, _, _, tmp_q \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_attr\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m         tmp_q \u001B[38;5;241m=\u001B[39m tmp_q\u001B[38;5;241m.\u001B[39mdata\n\u001B[0;32m     28\u001B[0m         p \u001B[38;5;241m=\u001B[39m target_distribution(tmp_q)\n",
      "File \u001B[1;32mD:\\Python\\anaconda3\\envs\\black\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[1;34m(self, x, edge_index_dict, edge_weight)\u001B[0m\n\u001B[0;32m     42\u001B[0m x1, edge_index1, edge_weight1 \u001B[38;5;241m=\u001B[39m aug1(x, edge_index, edge_weight)\n\u001B[0;32m     43\u001B[0m \u001B[38;5;66;03m# x2, edge_index2, edge_weight2 = aug2(x, edge_index, edge_weight)\u001B[39;00m\n\u001B[1;32m---> 45\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     47\u001B[0m z1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mencoder(x1, edge_index1, edge_weight1)\n\u001B[0;32m     48\u001B[0m \u001B[38;5;66;03m# z2 = self.encoder(x2, edge_index2, edge_weight2)\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Python\\anaconda3\\envs\\black\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [2]\u001B[0m, in \u001B[0;36mGConv.forward\u001B[1;34m(self, x, edge_index, edge_weight)\u001B[0m\n\u001B[0;32m     19\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayers[i](z, edge_index, edge_weight)\n\u001B[0;32m     20\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact(z)\n\u001B[1;32m---> 21\u001B[0m z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_weight\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m z\n",
      "File \u001B[1;32mD:\\Python\\anaconda3\\envs\\black\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[1;32mD:\\Python\\anaconda3\\envs\\black\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:246\u001B[0m, in \u001B[0;36mGATConv.forward\u001B[1;34m(self, x, edge_index, edge_attr, size, return_attention_weights)\u001B[0m\n\u001B[0;32m    243\u001B[0m alpha \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39medge_updater(edge_index, alpha\u001B[38;5;241m=\u001B[39malpha, edge_attr\u001B[38;5;241m=\u001B[39medge_attr)\n\u001B[0;32m    245\u001B[0m \u001B[38;5;66;03m# propagate_type: (x: OptPairTensor, alpha: Tensor)\u001B[39;00m\n\u001B[1;32m--> 246\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpropagate\u001B[49m\u001B[43m(\u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43malpha\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43malpha\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msize\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    248\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconcat:\n\u001B[0;32m    249\u001B[0m     out \u001B[38;5;241m=\u001B[39m out\u001B[38;5;241m.\u001B[39mview(\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mheads \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mout_channels)\n",
      "File \u001B[1;32mD:\\Python\\anaconda3\\envs\\black\\lib\\site-packages\\torch_geometric\\nn\\conv\\message_passing.py:374\u001B[0m, in \u001B[0;36mMessagePassing.propagate\u001B[1;34m(self, edge_index, size, **kwargs)\u001B[0m\n\u001B[0;32m    372\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m res \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    373\u001B[0m         msg_kwargs \u001B[38;5;241m=\u001B[39m res[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(res, \u001B[38;5;28mtuple\u001B[39m) \u001B[38;5;28;01melse\u001B[39;00m res\n\u001B[1;32m--> 374\u001B[0m out \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmessage\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mmsg_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    375\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m hook \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_message_forward_hooks\u001B[38;5;241m.\u001B[39mvalues():\n\u001B[0;32m    376\u001B[0m     res \u001B[38;5;241m=\u001B[39m hook(\u001B[38;5;28mself\u001B[39m, (msg_kwargs, ), out)\n",
      "File \u001B[1;32mD:\\Python\\anaconda3\\envs\\black\\lib\\site-packages\\torch_geometric\\nn\\conv\\gat_conv.py:285\u001B[0m, in \u001B[0;36mGATConv.message\u001B[1;34m(self, x_j, alpha)\u001B[0m\n\u001B[0;32m    284\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mmessage\u001B[39m(\u001B[38;5;28mself\u001B[39m, x_j: Tensor, alpha: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 285\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43malpha\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43munsqueeze\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mx_j\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 2.11 GiB (GPU 0; 8.00 GiB total capacity; 4.46 GiB already allocated; 0 bytes free; 6.53 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "\n",
    "aug1 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.1)])\n",
    "aug2 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.1)])\n",
    "\n",
    "gconv = GConv(input_dim=data.num_features, hidden_dim=256, num_layers=2).to(device)\n",
    "encoder_model = Encoder(encoder=gconv, augmentor=(aug1, aug2)).to(device)\n",
    "print(encoder_model.parameters())\n",
    "contrast_model = WithinEmbedContrast(loss=L.BarlowTwins()).to(device)\n",
    "\n",
    "optimizer = Adam(encoder_model.parameters(), lr=5e-4)\n",
    "\n",
    "losss = []\n",
    "kl_losss = []\n",
    "epoch = 1000\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z, _, _, q = encoder_model(data.x, data.edge_index, data.edge_attr)\n",
    "_ = kmeans.fit_predict(z.data.cpu().numpy())\n",
    "encoder_model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "\n",
    "with tqdm(total=epoch, desc='(T)') as pbar:\n",
    "    for epoch in range(1, epoch + 1):\n",
    "        if epoch % 1 == 0:\n",
    "            # update_interval\n",
    "            _, _, _, tmp_q = encoder_model(data.x, data.edge_index, data.edge_attr)\n",
    "            tmp_q = tmp_q.data\n",
    "            p = target_distribution(tmp_q)\n",
    "\n",
    "        loss, kl_loss = train(encoder_model, contrast_model, data, optimizer, p)\n",
    "        pbar.set_postfix({'loss': loss, 'kl_loss': kl_loss})\n",
    "        pbar.update()\n",
    "        losss.append(loss)\n",
    "        kl_losss.append(kl_loss)\n",
    "\n",
    "plt.plot(range(epoch), losss)\n",
    "plt.plot(range(epoch), kl_losss)\n",
    "plt.show()\n",
    "\n",
    "# test_result = test(encoder_model, data)\n",
    "# print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    encoder_model.eval()\n",
    "    z, _, _, _ = encoder_model(data.x, data.edge_index, data.edge_attr)\n",
    "\n",
    "pred = kmeans.fit_predict(z.cpu())\n",
    "\n",
    "nmi = normalized_mutual_info_score(pred, data.y.cpu())\n",
    "print('[INFO]NMI: ', nmi)\n",
    "\n",
    "ami = adjusted_mutual_info_score(pred, data.y.cpu())\n",
    "print('[INFO]AMI: ', ami)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pred = kmeans.fit_predict(x.cpu())\n",
    "\n",
    "nmi = normalized_mutual_info_score(pred, data.y.cpu())\n",
    "print('[INFO]NMI: ', nmi)\n",
    "\n",
    "ami = adjusted_mutual_info_score(pred, data.y.cpu())\n",
    "print('[INFO]AMI: ', ami)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

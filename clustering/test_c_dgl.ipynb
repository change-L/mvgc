{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzr/anaconda3/envs/clustering/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '12'\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import os.path as osp\n",
    "import GCL.losses as L\n",
    "import torch_geometric.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import GCL.augmentors as A\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, tensor\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from GCL.eval import get_split, LREvaluator, SVMEvaluator\n",
    "from GCL.models import SingleBranchContrast\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from GCL.models.contrast_model import WithinEmbedContrast\n",
    "from GCL.models import contrast_model\n",
    "from dgl.nn.pytorch import GATConv, GraphConv, GATv2Conv\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score\n",
    "from munkres import Munkres, print_matrix\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import dgl\n",
    "import dgl.function as fn\n",
    "import dgl.nn.pytorch as dglnn\n",
    "from dgl.utils import expand_as_pair\n",
    "\n",
    "class MyGATConv(nn.Module):\n",
    "    def __init__(self,\n",
    "                 in_feats,\n",
    "                 out_feats,\n",
    "                 num_heads,\n",
    "                 feat_drop=0.5,\n",
    "                 attn_drop=0.5,\n",
    "                 alpha=0.1,\n",
    "                 negative_slope=0.05,\n",
    "                 residual=True,\n",
    "                 activation=None,\n",
    "                 allow_zero_in_degree=False,\n",
    "                 bias=True,\n",
    "                 use_symmetric_norm=False\n",
    "                 ):\n",
    "        super(MyGATConv, self).__init__()\n",
    "        self._num_heads = num_heads\n",
    "        self._in_src_feats, self._in_dst_feats = expand_as_pair(in_feats)\n",
    "        self._out_feats = out_feats\n",
    "        self._allow_zero_in_degree = allow_zero_in_degree\n",
    "        self._use_symmetric_norm = use_symmetric_norm\n",
    "        self._residual = residual\n",
    "        if isinstance(in_feats, tuple):\n",
    "            self.fc_src = nn.Linear(\n",
    "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
    "            self.fc_dst = nn.Linear(\n",
    "                self._in_dst_feats, out_feats * num_heads, bias=False)\n",
    "        else:\n",
    "            self.fc = nn.Linear(\n",
    "                self._in_src_feats, out_feats * num_heads, bias=False)\n",
    "        self.attn_l = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
    "        self.attn_r = nn.Parameter(torch.FloatTensor(size=(1, num_heads, out_feats)))\n",
    "        self.feat_drop = nn.Dropout(feat_drop)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.alpha = alpha\n",
    "        self.leaky_relu = nn.LeakyReLU(negative_slope)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.FloatTensor(size=(num_heads * out_feats,)))\n",
    "        else:\n",
    "            self.register_buffer('bias', None)\n",
    "        if residual:\n",
    "            self.res_fc = nn.Linear(\n",
    "                self._in_dst_feats, num_heads * out_feats, bias=False)\n",
    "        else:\n",
    "            self.register_buffer('res_fc', None)\n",
    "        self.reset_parameters()\n",
    "        self.activation = activation\n",
    "\n",
    "        # edge_feats = out_feats\n",
    "        # self._edge_feats = edge_feats\n",
    "        # num_etypes = 4\n",
    "        # self.fc_edge = nn.Linear(\n",
    "        #     edge_feats, edge_feats * num_heads, bias=True)\n",
    "        # self.edge_emb = nn.Embedding(num_etypes, edge_feats)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        gain = nn.init.calculate_gain('relu')\n",
    "        if hasattr(self, 'fc'):\n",
    "            nn.init.xavier_normal_(self.fc.weight, gain=gain)\n",
    "        else:\n",
    "            nn.init.xavier_normal_(self.fc_src.weight, gain=gain)\n",
    "            nn.init.xavier_normal_(self.fc_dst.weight, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_l, gain=gain)\n",
    "        nn.init.xavier_normal_(self.attn_r, gain=gain)\n",
    "        nn.init.constant_(self.bias, 0)\n",
    "        if isinstance(self.res_fc, nn.Linear):\n",
    "            nn.init.xavier_normal_(self.res_fc.weight, gain=gain)\n",
    "\n",
    "    def forward(self, graph, feat, res_attn=None):\n",
    "        with graph.local_scope():\n",
    "            h_src = h_dst = self.feat_drop(feat)\n",
    "            feat_src = feat_dst = self.fc(h_src).view(\n",
    "                -1, self._num_heads, self._out_feats)\n",
    "            if graph.is_block:\n",
    "                feat_dst = feat_src[:graph.number_of_dst_nodes()]\n",
    "\n",
    "            if self._use_symmetric_norm:\n",
    "                degs = graph.out_degrees().float().clamp(min=1)\n",
    "                norm = torch.pow(degs, -0.5)\n",
    "                shp = norm.shape + (1,) * (feat_src.dim() - 1)\n",
    "                norm = torch.reshape(norm, shp)\n",
    "                feat_src = feat_src * norm\n",
    "\n",
    "            el = (feat_src * self.attn_l).sum(dim=-1).unsqueeze(-1)\n",
    "            er = (feat_dst * self.attn_r).sum(dim=-1).unsqueeze(-1)\n",
    "            graph.srcdata.update({'ft': feat_src, 'el': el})\n",
    "            graph.dstdata.update({'er': er})\n",
    "            # compute edge attention, el and er are a_l Wh_i and a_r Wh_j respectively.\n",
    "            graph.apply_edges(fn.u_add_v('el', 'er', 'e'))\n",
    "            e = self.leaky_relu(graph.edata.pop('e'))\n",
    "            if res_attn is not None:\n",
    "                e = e * (1 - self.alpha) + res_attn * self.alpha\n",
    "\n",
    "            # e = e.masked_fill(e < 0.05, 0)\n",
    "\n",
    "            # graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\n",
    "            e = self.attn_drop(dglnn.edge_softmax(graph, e))\n",
    "            graph.edata['a'] = e\n",
    "\n",
    "            # message passing\n",
    "            graph.update_all(fn.u_mul_e('ft', 'a', 'm'),\n",
    "                             fn.sum('m', 'ft'))\n",
    "            rst = graph.dstdata['ft']\n",
    "\n",
    "            if self._use_symmetric_norm:\n",
    "                degs = graph.out_degrees().float().clamp(min=1)\n",
    "                norm = torch.pow(degs, -0.5)\n",
    "                shp = norm.shape + (1,) * (feat_src.dim() - 1)\n",
    "                norm = torch.reshape(norm, shp)\n",
    "                rst = rst * norm\n",
    "            # residual\n",
    "            if self.res_fc is not None:\n",
    "                resval = self.res_fc(h_dst).view(h_dst.shape[0], self._num_heads, self._out_feats)\n",
    "                rst = rst + resval\n",
    "\n",
    "            # bias\n",
    "            if self.bias is not None:\n",
    "                rst = rst + self.bias.view(1, self._num_heads, self._out_feats)\n",
    "            # activation\n",
    "            if self.activation:\n",
    "                rst = self.activation(rst)\n",
    "\n",
    "            return rst, graph.edata['a'].detach()\n",
    "\n",
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, num_heads=1):\n",
    "        super(GConv, self).__init__()\n",
    "        self.act = nn.SELU()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = nn.BatchNorm1d(num_heads * hidden_dim)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            MyGATConv(in_feats=input_dim, out_feats=hidden_dim, allow_zero_in_degree=True, num_heads=num_heads, residual=False)\n",
    "        )\n",
    "        for _ in range(1, num_layers):\n",
    "            self.layers.append(\n",
    "                MyGATConv(in_feats=num_heads * hidden_dim, out_feats=hidden_dim, allow_zero_in_degree=True, num_heads=num_heads, residual=False)\n",
    "            )\n",
    "        self.register_buffer(\"epsilon\", torch.FloatTensor([1e-12]))\n",
    "\n",
    "    def forward(self, x, graph):\n",
    "        z = x\n",
    "        attn = None\n",
    "        for i in range(self.num_layers):\n",
    "            z, attn = self.layers[i](graph, z, attn)\n",
    "            z = z.flatten(1)\n",
    "            z = self.norm(z)\n",
    "            z = self.act(z)\n",
    "        z = z / (torch.max(torch.norm(z, dim=1, keepdim=True), self.epsilon))\n",
    "        # z = self.layers[-1](graph, z)\n",
    "        return z\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, graph, augmentor, hidden_dim=256, n_clusters=3, num_heads=1, num_layers=3):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "\n",
    "        self.encoder = GConv(input_dim=x.size(1), hidden_dim=hidden_dim, num_layers=num_layers, num_heads=num_heads).to(device)\n",
    "\n",
    "        # self.decoder = nn.Sequential(\n",
    "        #     nn.Linear(num_heads * hidden_dim, num_heads * hidden_dim),\n",
    "        #     nn.SELU(),\n",
    "        #     nn.Linear(num_heads * hidden_dim, num_heads * hidden_dim),\n",
    "        #     nn.SELU(),\n",
    "        # )\n",
    "\n",
    "        self.augmentor = augmentor\n",
    "        self.register_buffer(\"epsilon\", torch.FloatTensor([1e-12]))\n",
    "\n",
    "        self.cluster_layer = nn.Parameter(torch.Tensor(n_clusters, num_heads * hidden_dim))\n",
    "        self.v = 1\n",
    "        self.device = torch.device('cuda')\n",
    "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "        self.alpha = 0.5\n",
    "\n",
    "        # self.pe = dgl.laplacian_pe(graph, x.size(1)).to(device)\n",
    "\n",
    "    def forward(self, x, graph, edge_index, edge_weight=None):\n",
    "        aug1, aug2 = self.augmentor\n",
    "        z = self.encoder(x, graph)\n",
    "        # z = self.decoder(z)\n",
    "\n",
    "        x1, edge_index1, _ = aug1(x, edge_index)\n",
    "        graph1 = dgl.graph((edge_index1[0], edge_index1[1]), num_nodes=graph.num_nodes()).to(self.device)\n",
    "        z1 = self.encoder(x1, graph1)\n",
    "\n",
    "        x2, edge_index2, _ = aug2(x, edge_index)\n",
    "        graph2 = dgl.graph((edge_index2[0], edge_index2[1]), num_nodes=graph.num_nodes()).to(self.device)\n",
    "        z2 = self.encoder(x2, graph2)\n",
    "\n",
    "\n",
    "        q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "        q = q.pow((self.v + 1.0) / 2.0)\n",
    "        q = (q.t() / torch.sum(q, 1)).t()\n",
    "\n",
    "        # qs.append(q)\n",
    "        return z, z1, z2, q\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'PAP', 'PLP', 'PMP', 'PTP', 'feature', 'label', 'test_idx', 'train_idx', 'val_idx'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda')\n",
    "# path = osp.join(osp.pardir, 'datasets', 'DBLP')\n",
    "#\n",
    "# mat = loadmat(osp.join(path, 'DBLP4057_GAT_with_idx.mat'))\n",
    "\n",
    "\n",
    "path = osp.join(osp.pardir, 'datasets', 'ACM')\n",
    "\n",
    "mat = loadmat(osp.join(path, 'ACM3025.mat'))\n",
    "print(mat.keys())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(mat['train_idx'][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████| 3025/3025 [00:12<00:00, 241.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29281\n",
      "torch.Size([2, 29281])\n"
     ]
    }
   ],
   "source": [
    "# edge_index = []\n",
    "# for i in tqdm(range(len(mat['PLP']))):\n",
    "#     for j in range(len(mat['PLP'])):\n",
    "#         if mat['PAP'][i][j] == 1:\n",
    "#             edge_index.append([i, j])\n",
    "\n",
    "graph_dict = {}\n",
    "# for etype in ['net_APCPA']:\n",
    "for etype in ['PAP']:\n",
    "    edge_index = []\n",
    "    for i in tqdm(range(len(mat[etype]))):\n",
    "        for j in range(len(mat[etype])):\n",
    "            if mat[etype][i][j] == 1:\n",
    "                edge_index.append([i, j])\n",
    "    print(len(edge_index))\n",
    "    edge_index = tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    graph = dgl.graph((edge_index[0], edge_index[1])).to(device)\n",
    "    graph.remove_self_loop()\n",
    "    graph.add_self_loop()\n",
    "\n",
    "    edge_index = torch.stack(graph.edges())\n",
    "    print(edge_index.size())\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600])\n"
     ]
    }
   ],
   "source": [
    "x = tensor(mat['feature'], dtype=torch.float).to(device)\n",
    "y = torch.argmax(tensor(mat['label']), -1).to(device)\n",
    "train_idx = tensor(mat['train_idx'], dtype=torch.int64).flatten().to(device)\n",
    "val_idx = tensor(mat['val_idx'], dtype=torch.int64).flatten().to(device)\n",
    "test_idx = tensor(mat['test_idx'], dtype=torch.int64).flatten().to(device)\n",
    "print(train_idx.size())\n",
    "train_mask = torch.index_fill(torch.zeros_like(y), index=train_idx, value=1, dim=0)\n",
    "val_mask = torch.index_fill(torch.zeros_like(y), index=val_idx, value=1, dim=0)\n",
    "test_mask = torch.index_fill(torch.zeros_like(y), index=test_idx, value=1, dim=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "del mat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model have 290688 paramerters in total\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/lzr/anaconda3/envs/clustering/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "(T):   0%|                                                                         | 0/1000 [00:00<?, ?it/s]/home/lzr/anaconda3/envs/clustering/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "(T):   0%|                                | 0/1000 [00:00<?, ?it/s, loss=115, con_loss=115, kl_loss=0.00138]/home/lzr/anaconda3/envs/clustering/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "(T):   2%|▌                    | 25/1000 [00:01<00:48, 19.90it/s, loss=94.5, con_loss=94.1, kl_loss=0.00407]/home/lzr/anaconda3/envs/clustering/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "(T):   5%|█                     | 51/1000 [00:02<00:44, 21.09it/s, loss=76.5, con_loss=75.4, kl_loss=0.0102]/home/lzr/anaconda3/envs/clustering/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "(T):   8%|█▋                    | 75/1000 [00:03<00:45, 20.44it/s, loss=66.2, con_loss=64.2, kl_loss=0.0201]/home/lzr/anaconda3/envs/clustering/lib/python3.8/site-packages/torch_geometric/deprecation.py:12: UserWarning: 'dropout_adj' is deprecated, use 'dropout_edge' instead\n",
      "  warnings.warn(out)\n",
      "(T):   8%|█▊                    | 80/1000 [00:04<00:49, 18.73it/s, loss=64.7, con_loss=62.5, kl_loss=0.0217]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[9], line 135\u001B[0m\n\u001B[1;32m    128\u001B[0m encoder_model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m    129\u001B[0m \u001B[38;5;66;03m# if epoch % 1 == 0:\u001B[39;00m\n\u001B[1;32m    130\u001B[0m \u001B[38;5;66;03m#     # update_interval\u001B[39;00m\n\u001B[1;32m    131\u001B[0m \u001B[38;5;66;03m#     _, _, _, tmp_q = encoder_model(x, graph, edge_index)\u001B[39;00m\n\u001B[1;32m    132\u001B[0m \u001B[38;5;66;03m#\u001B[39;00m\n\u001B[1;32m    133\u001B[0m \u001B[38;5;66;03m#     p = target_distribution(tmp_q.data)\u001B[39;00m\n\u001B[0;32m--> 135\u001B[0m loss, con_loss, kl_loss \u001B[38;5;241m=\u001B[39m \u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43mencoder_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcontrast_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moptimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    136\u001B[0m pbar\u001B[38;5;241m.\u001B[39mset_postfix({\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mloss\u001B[39m\u001B[38;5;124m'\u001B[39m: loss, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcon_loss\u001B[39m\u001B[38;5;124m'\u001B[39m: con_loss, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mkl_loss\u001B[39m\u001B[38;5;124m'\u001B[39m: kl_loss})\n\u001B[1;32m    137\u001B[0m pbar\u001B[38;5;241m.\u001B[39mupdate()\n",
      "Cell \u001B[0;32mIn[9], line 60\u001B[0m, in \u001B[0;36mtrain\u001B[0;34m(encoder_model, contrast_model, optimizer, x, graph, edge_index)\u001B[0m\n\u001B[1;32m     58\u001B[0m encoder_model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m     59\u001B[0m optimizer\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m---> 60\u001B[0m _, z1, z2, q \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43medge_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     61\u001B[0m p \u001B[38;5;241m=\u001B[39m target_distribution(q\u001B[38;5;241m.\u001B[39mdata)\n\u001B[1;32m     62\u001B[0m con_loss \u001B[38;5;241m=\u001B[39m contrast_model(z1, z2)\n",
      "File \u001B[0;32m~/anaconda3/envs/clustering/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[2], line 191\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[0;34m(self, x, graph, edge_index, edge_weight)\u001B[0m\n\u001B[1;32m    189\u001B[0m x1, edge_index1, _ \u001B[38;5;241m=\u001B[39m aug1(x, edge_index)\n\u001B[1;32m    190\u001B[0m graph1 \u001B[38;5;241m=\u001B[39m dgl\u001B[38;5;241m.\u001B[39mgraph((edge_index1[\u001B[38;5;241m0\u001B[39m], edge_index1[\u001B[38;5;241m1\u001B[39m]), num_nodes\u001B[38;5;241m=\u001B[39mgraph\u001B[38;5;241m.\u001B[39mnum_nodes())\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[0;32m--> 191\u001B[0m z1 \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mencoder\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx1\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph1\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    193\u001B[0m x2, edge_index2, _ \u001B[38;5;241m=\u001B[39m aug2(x, edge_index)\n\u001B[1;32m    194\u001B[0m graph2 \u001B[38;5;241m=\u001B[39m dgl\u001B[38;5;241m.\u001B[39mgraph((edge_index2[\u001B[38;5;241m0\u001B[39m], edge_index2[\u001B[38;5;241m1\u001B[39m]), num_nodes\u001B[38;5;241m=\u001B[39mgraph\u001B[38;5;241m.\u001B[39mnum_nodes())\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdevice)\n",
      "File \u001B[0;32m~/anaconda3/envs/clustering/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[2], line 150\u001B[0m, in \u001B[0;36mGConv.forward\u001B[0;34m(self, x, graph)\u001B[0m\n\u001B[1;32m    148\u001B[0m attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    149\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_layers):\n\u001B[0;32m--> 150\u001B[0m     z, attn \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayers\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mz\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    151\u001B[0m     z \u001B[38;5;241m=\u001B[39m z\u001B[38;5;241m.\u001B[39mflatten(\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    152\u001B[0m     z \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnorm(z)\n",
      "File \u001B[0;32m~/anaconda3/envs/clustering/lib/python3.8/site-packages/torch/nn/modules/module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Cell \u001B[0;32mIn[2], line 102\u001B[0m, in \u001B[0;36mMyGATConv.forward\u001B[0;34m(self, graph, feat, res_attn)\u001B[0m\n\u001B[1;32m     97\u001B[0m     e \u001B[38;5;241m=\u001B[39m e \u001B[38;5;241m*\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha) \u001B[38;5;241m+\u001B[39m res_attn \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39malpha\n\u001B[1;32m     99\u001B[0m \u001B[38;5;66;03m# e = e.masked_fill(e < 0.05, 0)\u001B[39;00m\n\u001B[1;32m    100\u001B[0m \n\u001B[1;32m    101\u001B[0m \u001B[38;5;66;03m# graph.edata['a'] = self.attn_drop(edge_softmax(graph, e))\u001B[39;00m\n\u001B[0;32m--> 102\u001B[0m e \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mattn_drop(\u001B[43mdglnn\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43medge_softmax\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgraph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[1;32m    103\u001B[0m graph\u001B[38;5;241m.\u001B[39medata[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124ma\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m e\n\u001B[1;32m    105\u001B[0m \u001B[38;5;66;03m# message passing\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/clustering/lib/python3.8/site-packages/dgl/ops/edge_softmax.py:134\u001B[0m, in \u001B[0;36medge_softmax\u001B[0;34m(graph, logits, eids, norm_by)\u001B[0m\n\u001B[1;32m    132\u001B[0m     eids \u001B[38;5;241m=\u001B[39m astype(eids, graph\u001B[38;5;241m.\u001B[39midtype)\n\u001B[1;32m    133\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m graph\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39mnumber_of_etypes() \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m--> 134\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43medge_softmax_internal\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[43m        \u001B[49m\u001B[43mgraph\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_graph\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43meids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meids\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnorm_by\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnorm_by\u001B[49m\n\u001B[1;32m    136\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    137\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    138\u001B[0m     logits_list \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28;01mNone\u001B[39;00m] \u001B[38;5;241m*\u001B[39m graph\u001B[38;5;241m.\u001B[39m_graph\u001B[38;5;241m.\u001B[39mnumber_of_etypes()\n",
      "File \u001B[0;32m~/anaconda3/envs/clustering/lib/python3.8/site-packages/dgl/backend/pytorch/sparse.py:1116\u001B[0m, in \u001B[0;36medge_softmax\u001B[0;34m(gidx, logits, eids, norm_by)\u001B[0m\n\u001B[1;32m   1114\u001B[0m args \u001B[38;5;241m=\u001B[39m _cast_if_autocast_enabled(gidx, logits, eids, norm_by)\n\u001B[1;32m   1115\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m _disable_autocast_if_enabled():\n\u001B[0;32m-> 1116\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mEdgeSoftmax\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/envs/clustering/lib/python3.8/site-packages/dgl/backend/pytorch/sparse.py:710\u001B[0m, in \u001B[0;36mEdgeSoftmax.forward\u001B[0;34m(ctx, gidx, score, eids, norm_by)\u001B[0m\n\u001B[1;32m    707\u001B[0m \u001B[38;5;66;03m# Note: Now _edge_softmax_forward op only supports CPU\u001B[39;00m\n\u001B[1;32m    708\u001B[0m \u001B[38;5;66;03m# TODO(Zhejiang): We will support GPU in the future\u001B[39;00m\n\u001B[1;32m    709\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m score\u001B[38;5;241m.\u001B[39mis_cuda:\n\u001B[0;32m--> 710\u001B[0m     score_max \u001B[38;5;241m=\u001B[39m \u001B[43m_gspmm\u001B[49m\u001B[43m(\u001B[49m\u001B[43mgidx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mcopy_rhs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmax\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mscore\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m    711\u001B[0m     score \u001B[38;5;241m=\u001B[39m th\u001B[38;5;241m.\u001B[39mexp(_gsddmm(gidx, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msub\u001B[39m\u001B[38;5;124m\"\u001B[39m, score, score_max, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124me\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mv\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[1;32m    712\u001B[0m     score_sum \u001B[38;5;241m=\u001B[39m _gspmm(gidx, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcopy_rhs\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m, score)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/anaconda3/envs/clustering/lib/python3.8/site-packages/dgl/_sparse_ops.py:215\u001B[0m, in \u001B[0;36m_gspmm\u001B[0;34m(gidx, op, reduce_op, u, e)\u001B[0m\n\u001B[1;32m    213\u001B[0m         expand_u \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    214\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m use_e:\n\u001B[0;32m--> 215\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mndim\u001B[49m\u001B[43m(\u001B[49m\u001B[43me\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    216\u001B[0m         e \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39munsqueeze(e, \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[1;32m    217\u001B[0m         expand_e \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/clustering/lib/python3.8/site-packages/dgl/backend/pytorch/tensor.py:94\u001B[0m, in \u001B[0;36mndim\u001B[0;34m(input)\u001B[0m\n\u001B[1;32m     90\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mdtype\u001B[39m(\u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m     91\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdtype\n\u001B[0;32m---> 94\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mndim\u001B[39m(\u001B[38;5;28minput\u001B[39m):\n\u001B[1;32m     95\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\u001B[38;5;241m.\u001B[39mdim()\n\u001B[1;32m     98\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcontext\u001B[39m(\u001B[38;5;28minput\u001B[39m):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    y_true = y_true - np.min(y_true)\n",
    "\n",
    "    l1 = list(set(y_true))\n",
    "    numclass1 = len(l1)\n",
    "\n",
    "    l2 = list(set(y_pred))\n",
    "    numclass2 = len(l2)\n",
    "\n",
    "    ind = 0\n",
    "    if numclass1 != numclass2:\n",
    "        for i in l1:\n",
    "            if i in l2:\n",
    "                pass\n",
    "            else:\n",
    "                y_pred[ind] = i\n",
    "                ind += 1\n",
    "\n",
    "    l2 = list(set(y_pred))\n",
    "    numclass2 = len(l2)\n",
    "\n",
    "    if numclass1 != numclass2:\n",
    "        print('error')\n",
    "        return\n",
    "\n",
    "    cost = np.zeros((numclass1, numclass2), dtype=int)\n",
    "    for i, c1 in enumerate(l1):\n",
    "        mps = [i1 for i1, e1 in enumerate(y_true) if e1 == c1]\n",
    "        for j, c2 in enumerate(l2):\n",
    "            mps_d = [i1 for i1 in mps if y_pred[i1] == c2]\n",
    "            cost[i][j] = len(mps_d)\n",
    "\n",
    "    # match two clustering results by Munkres algorithm\n",
    "    m = Munkres()\n",
    "    cost = cost.__neg__().tolist()\n",
    "    indexes = m.compute(cost)\n",
    "\n",
    "    # get the match results\n",
    "    new_predict = np.zeros(len(y_pred))\n",
    "    for i, c in enumerate(l1):\n",
    "        # correponding label in l2:\n",
    "        c2 = l2[indexes[i][1]]\n",
    "\n",
    "        # ai is the index with label==c2 in the pred_label list\n",
    "        ai = [ind for ind, elm in enumerate(y_pred) if elm == c2]\n",
    "        new_predict[ai] = c\n",
    "\n",
    "    acc = metrics.accuracy_score(y_true, new_predict)\n",
    "    f1_macro = metrics.f1_score(y_true, new_predict, average='macro')\n",
    "    precision_macro = metrics.precision_score(y_true, new_predict, average='macro')\n",
    "    recall_macro = metrics.recall_score(y_true, new_predict, average='macro')\n",
    "    f1_micro = metrics.f1_score(y_true, new_predict, average='micro')\n",
    "    precision_micro = metrics.precision_score(y_true, new_predict, average='micro')\n",
    "    recall_micro = metrics.recall_score(y_true, new_predict, average='micro')\n",
    "    return acc, f1_macro\n",
    "\n",
    "def train(encoder_model, contrast_model, optimizer, x, graph, edge_index):\n",
    "    encoder_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    _, z1, z2, q = encoder_model(x, graph, edge_index)\n",
    "    p = target_distribution(q.data)\n",
    "    con_loss = contrast_model(z1, z2)\n",
    "\n",
    "    kl_loss = F.kl_div(q.log(), p, reduction='batchmean')\n",
    "\n",
    "    # con_loss = 0.01 * con_loss\n",
    "    loss = (con_loss + 100 * kl_loss)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), con_loss.item(), kl_loss.item()\n",
    "\n",
    "aug1 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.3)])\n",
    "aug2 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.3)])\n",
    "\n",
    "class InfoNCE(nn.Module):\n",
    "    def __init__(self, tau):\n",
    "        super(InfoNCE, self).__init__()\n",
    "        self.tau = tau\n",
    "\n",
    "    def sim(self, z1, z2):\n",
    "        # normalize embeddings across feature dimension\n",
    "        z1 = F.normalize(z1)\n",
    "        z2 = F.normalize(z2)\n",
    "\n",
    "        s = torch.mm(z1, z2.t())\n",
    "        return s\n",
    "\n",
    "    def get_loss(self, z1, z2):\n",
    "        # calculate SimCLR loss\n",
    "        f = lambda x: torch.exp(x / self.tau)\n",
    "\n",
    "        refl_sim = f(self.sim(z1, z1))  # intra-view pairs\n",
    "        between_sim = f(self.sim(z1, z2))  # inter-view pairs\n",
    "\n",
    "        # between_sim.diag(): positive pairs\n",
    "        x1 = refl_sim.sum(1) + between_sim.sum(1) - refl_sim.diag()\n",
    "        loss = -torch.log(between_sim.diag() / x1)\n",
    "\n",
    "        return loss.mean()\n",
    "\n",
    "    def forward(self, z1, z2):\n",
    "        l1 = self.get_loss(z1, z2)\n",
    "        l2 = self.get_loss(z2, z1)\n",
    "\n",
    "        ret = (l1 + l2) * 0.5\n",
    "\n",
    "        # return ret.mean()\n",
    "        return ret\n",
    "\n",
    "#0.9\n",
    "# aug1 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.1)])\n",
    "# aug2 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.1)])\n",
    "\n",
    "#0.90\n",
    "# aug1 = A.Compose([A.EdgeRemoving(pe=0.3), A.FeatureDropout(pf=0.3)])\n",
    "# aug2 = A.Compose([A.EdgeRemoving(pe=0.3), A.FeatureDropout(pf=0.3)])\n",
    "\n",
    "# aug1 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureDropout(pf=0.5)])\n",
    "# aug2 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureDropout(pf=0.5)])\n",
    "\n",
    "# aug1 = A.RandomChoice([A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "#                       A.NodeDropping(pn=0.1),\n",
    "#                       A.FeatureMasking(pf=0.1),\n",
    "#                       A.EdgeRemoving(pe=0.1)],\n",
    "#                      num_choices=1)\n",
    "# aug2 = A.RandomChoice([A.RWSampling(num_seeds=1000, walk_length=10),\n",
    "#                       A.NodeDropping(pn=0.1),\n",
    "#                       A.FeatureMasking(pf=0.1),\n",
    "#                       A.EdgeRemoving(pe=0.1)],\n",
    "#                      num_choices=1)\n",
    "\n",
    "# encoder_model = Encoder(graph=graph, augmentor=(aug1, aug2), hidden_dim=32, num_heads=4, n_clusters=4, num_layers=2).to(device)\n",
    "encoder_model = Encoder(graph=graph, augmentor=(aug1, aug2), hidden_dim=32, num_heads=4, n_clusters=3, num_layers=4).to(device)\n",
    "\n",
    "print(\"model have {} paramerters in total\".format(sum(x.numel() for x in encoder_model.parameters())))\n",
    "\n",
    "# contrast_model = WithinEmbedContrast(loss=L.BarlowTwins()).to(device)\n",
    "contrast_model = InfoNCE(tau=0.2)\n",
    "# contrast_model = WithinEmbedContrast(loss=L.VICReg()).to(device)\n",
    "\n",
    "# optimizer = Adam(encoder_model.parameters(), lr=5e-4)\n",
    "optimizer = Adam(encoder_model.parameters(), lr=0.0003)\n",
    "losss = []\n",
    "kl_losss = []\n",
    "con_losss = []\n",
    "nmis = []\n",
    "aris = []\n",
    "ratios = []\n",
    "accs = []\n",
    "f1s = []\n",
    "epochs = 1000\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z, _, _, _ = encoder_model(x, graph, edge_index)\n",
    "_ = kmeans.fit_predict(z.data.cpu().numpy())\n",
    "encoder_model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "\n",
    "with tqdm(total=epochs, desc='(T)') as pbar:\n",
    "    for epoch in range(epochs):\n",
    "        encoder_model.train()\n",
    "        # if epoch % 1 == 0:\n",
    "        #     # update_interval\n",
    "        #     _, _, _, tmp_q = encoder_model(x, graph, edge_index)\n",
    "        #\n",
    "        #     p = target_distribution(tmp_q.data)\n",
    "\n",
    "        loss, con_loss, kl_loss = train(encoder_model, contrast_model, optimizer, x, graph, edge_index)\n",
    "        pbar.set_postfix({'loss': loss, 'con_loss': con_loss, 'kl_loss': kl_loss})\n",
    "        pbar.update()\n",
    "        losss.append(loss)\n",
    "        kl_losss.append(kl_loss)\n",
    "        con_losss.append(con_loss)\n",
    "        ratios.append((100 * kl_loss) / loss)\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            with torch.no_grad():\n",
    "                encoder_model.eval()\n",
    "                z, _, _, _ = encoder_model(x, graph, edge_index)\n",
    "            pred = kmeans.fit_predict(z.cpu())\n",
    "            nmi = normalized_mutual_info_score(pred, y.cpu())\n",
    "            ari = adjusted_rand_score(pred, y.cpu())\n",
    "            nmis.append((epoch, nmi))\n",
    "            aris.append((epoch, ari))\n",
    "\n",
    "            acc, f1 = cluster_acc(y.cpu().numpy(), pred)\n",
    "\n",
    "            accs.append((epoch, acc))\n",
    "            f1s.append((epoch, f1))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 8))\n",
    "axes[0][0].plot(range(epochs), losss)\n",
    "axes[0][1].plot(range(epochs), kl_losss)\n",
    "axes[0][2].plot(range(epochs), con_losss)\n",
    "axes[1][1].plot([i[0] for i in nmis], [i[1] for i in nmis])\n",
    "axes[1][1].plot([i[0] for i in aris], [i[1] for i in aris])\n",
    "axes[1][2].plot([i[0] for i in aris], [i[1] for i in accs])\n",
    "axes[1][2].plot([i[0] for i in aris], [i[1] for i in f1s])\n",
    "plt.show()\n",
    "print('[MAX]ACC: ', max([i[1] for i in accs]))\n",
    "print('[MAX]NMI: ', max([i[1] for i in nmis]))\n",
    "print('[MAX]ARI: ', max([i[1] for i in aris]))\n",
    "print('[MAX]F1: ', max([i[1] for i in f1s]))\n",
    "# test_result = test(encoder_model, data)\n",
    "# print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    encoder_model.eval()\n",
    "    z, _, _, qs = encoder_model(x, graph, edge_index)\n",
    "\n",
    "pred = kmeans.fit_predict(z.cpu())\n",
    "\n",
    "nmi = normalized_mutual_info_score(pred, y.cpu())\n",
    "print('[INFO]NMI: ', nmi)\n",
    "\n",
    "ari = adjusted_rand_score(pred, y.cpu())\n",
    "print('[INFO]ARI: ', ari)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pred = kmeans.fit_predict(x.cpu())\n",
    "\n",
    "nmi = normalized_mutual_info_score(pred, y.cpu())\n",
    "print('[INFO]NMI: ', nmi)\n",
    "\n",
    "ari = adjusted_rand_score(pred, y.cpu())\n",
    "print('[INFO]ARI: ', ari)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_model.eval()\n",
    "    z, _, _, _ = encoder_model(x, graph_dict)\n",
    "\n",
    "pred = kmeans.fit_predict(z.cpu())\n",
    "acc, f1 = cluster_acc(y.cpu().numpy(), pred)\n",
    "print(acc, f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

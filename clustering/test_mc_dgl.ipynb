{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OMP_NUM_THREADS\"] = '12'\n",
    "\n",
    "import torch\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import dgl\n",
    "import os.path as osp\n",
    "import GCL.losses as L\n",
    "import torch_geometric.transforms as T\n",
    "import matplotlib.pyplot as plt\n",
    "import GCL.augmentors as A\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn, tensor\n",
    "from tqdm import tqdm\n",
    "from torch.optim import Adam\n",
    "from GCL.eval import get_split, LREvaluator, SVMEvaluator\n",
    "from GCL.models import SingleBranchContrast\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from GCL.models.contrast_model import WithinEmbedContrast\n",
    "from dgl.nn.pytorch import GATConv, GraphConv\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import normalized_mutual_info_score, adjusted_rand_score\n",
    "from munkres import Munkres, print_matrix\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [],
   "source": [
    "class GConv(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, num_layers=2, num_heads=1):\n",
    "        super(GConv, self).__init__()\n",
    "        self.act = nn.SELU()\n",
    "        self.num_layers = num_layers\n",
    "        self.norm = nn.BatchNorm1d(hidden_dim, momentum=0.01)\n",
    "        self.layers = nn.ModuleList()\n",
    "        self.layers.append(\n",
    "            GraphConv(in_feats=input_dim, out_feats=hidden_dim, allow_zero_in_degree=True)\n",
    "        )\n",
    "        for _ in range(1, num_layers):\n",
    "            self.layers.append(\n",
    "                GraphConv(in_feats=hidden_dim, out_feats=hidden_dim, allow_zero_in_degree=True)\n",
    "            )\n",
    "        self.register_buffer(\"epsilon\", torch.FloatTensor([1e-12]))\n",
    "\n",
    "    def forward(self, x, graph):\n",
    "        z = x\n",
    "        for i in range(self.num_layers):\n",
    "            z = self.layers[i](graph, z)\n",
    "            z = z.flatten(1)\n",
    "            z = self.norm(z)\n",
    "            z = self.act(z)\n",
    "        # z = z / (torch.max(torch.norm(z, dim=1, keepdim=True), self.epsilon))\n",
    "        # z = self.layers[-1](graph, z)\n",
    "        return z\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, graph_dict, augmentor, hidden_dim=256, n_clusters=3, num_heads=1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        gconvs = nn.ModuleDict()\n",
    "\n",
    "        for k, v in graph_dict.items():\n",
    "            gconv = GConv(input_dim=x.size(1), hidden_dim=hidden_dim, num_layers=3, num_heads=num_heads).to(device)\n",
    "            gconvs[k] = gconv\n",
    "        self.encoder = gconvs\n",
    "        self.augmentor = augmentor\n",
    "        self.register_buffer(\"epsilon\", torch.FloatTensor([1e-12]))\n",
    "\n",
    "        self.cluster_layer = nn.Parameter(torch.Tensor(n_clusters, num_heads * hidden_dim))\n",
    "        self.v = 1\n",
    "        self.device = torch.device('cuda')\n",
    "        torch.nn.init.xavier_normal_(self.cluster_layer.data)\n",
    "\n",
    "        self.alpha = 0.5\n",
    "\n",
    "        self.pe = {}\n",
    "        for key, (graph, edge_index) in graph_dict.items():\n",
    "            self.pe[key] = dgl.laplacian_pe(graph, x.size(1)).to(device)\n",
    "\n",
    "    def forward(self, x, graph_dict, edge_weight=None):\n",
    "        aug1, aug2 = self.augmentor\n",
    "        zs = []\n",
    "        z1s = []\n",
    "        z2s = []\n",
    "        qs = []\n",
    "        for key, (graph, edge_index) in graph_dict.items():\n",
    "            z = self.encoder[key](x + self.pe[key], graph)\n",
    "\n",
    "            x1, edge_index1, _ = aug1(x, edge_index)\n",
    "            graph1 = dgl.graph((edge_index1[0], edge_index1[1]), num_nodes=graph.num_nodes()).to(self.device)\n",
    "            z1 = self.encoder[key](x1, graph1)\n",
    "\n",
    "            x2, edge_index2, _ = aug2(x, edge_index)\n",
    "            graph2 = dgl.graph((edge_index2[0], edge_index2[1]), num_nodes=graph.num_nodes()).to(self.device)\n",
    "            z2 = self.encoder[key](x2, graph2)\n",
    "\n",
    "            zs.append(z)\n",
    "            z1s.append(z1)\n",
    "            z2s.append(z2)\n",
    "\n",
    "            # z = self.alpha * zs[0] + (1 - self.alpha) * zs[1]\n",
    "            # z = zs[0]\n",
    "            q = 1.0 / (1.0 + torch.sum(torch.pow(z.unsqueeze(1) - self.cluster_layer, 2), 2) / self.v)\n",
    "            q = q.pow((self.v + 1.0) / 2.0)\n",
    "            q = (q.t() / torch.sum(q, 1)).t()\n",
    "\n",
    "            qs.append(q)\n",
    "        z = zs[0] + zs[1]\n",
    "        z = z / (torch.max(torch.norm(z, dim=1, keepdim=True), self.epsilon))\n",
    "\n",
    "        return z, z1s, z2s, qs\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.t() / weight.sum(1)).t()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['__header__', '__version__', '__globals__', 'PAP', 'PLP', 'PMP', 'PTP', 'feature', 'label', 'test_idx', 'train_idx', 'val_idx'])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device('cuda')\n",
    "path = osp.join(osp.pardir, 'datasets', 'ACM')\n",
    "\n",
    "mat = loadmat(osp.join(path, 'ACM3025.mat'))\n",
    "print(mat.keys())\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "600\n"
     ]
    }
   ],
   "source": [
    "print(len(mat['train_idx'][0]))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [00:08<00:00, 347.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3025/3025 [00:10<00:00, 301.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2210761\n"
     ]
    }
   ],
   "source": [
    "# edge_index = []\n",
    "# for i in tqdm(range(len(mat['PLP']))):\n",
    "#     for j in range(len(mat['PLP'])):\n",
    "#         if mat['PAP'][i][j] == 1:\n",
    "#             edge_index.append([i, j])\n",
    "\n",
    "graph_dict = {}\n",
    "for etype in ['PAP', 'PLP']:\n",
    "    edge_index = []\n",
    "    for i in tqdm(range(len(mat[etype]))):\n",
    "        for j in range(len(mat[etype])):\n",
    "            if mat[etype][i][j] == 1:\n",
    "                edge_index.append([i, j])\n",
    "    print(len(edge_index))\n",
    "    edge_index = tensor(edge_index, dtype=torch.long).t().contiguous()\n",
    "    graph = dgl.graph((edge_index[0], edge_index[1])).to(device)\n",
    "    graph_dict[etype] = (graph, edge_index)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'PAP': (Graph(num_nodes=3025, num_edges=29281,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={}), tensor([[   0,    0,    0,  ..., 3024, 3024, 3024],\n",
      "        [   0,    8,   20,  ..., 2983, 2991, 3024]])), 'PLP': (Graph(num_nodes=3025, num_edges=2210761,\n",
      "      ndata_schemes={}\n",
      "      edata_schemes={}), tensor([[   0,    0,    0,  ..., 3024, 3024, 3024],\n",
      "        [   0,   75,  434,  ..., 3021, 3022, 3024]]))}\n"
     ]
    }
   ],
   "source": [
    "print(graph_dict)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600])\n"
     ]
    }
   ],
   "source": [
    "x = tensor(mat['feature'], dtype=torch.float).to(device)\n",
    "y = torch.argmax(tensor(mat['label']), -1).to(device)\n",
    "train_idx = tensor(mat['train_idx'], dtype=torch.int64).flatten().to(device)\n",
    "val_idx = tensor(mat['val_idx'], dtype=torch.int64).flatten().to(device)\n",
    "test_idx = tensor(mat['test_idx'], dtype=torch.int64).flatten().to(device)\n",
    "print(train_idx.size())\n",
    "train_mask = torch.index_fill(torch.zeros_like(y), index=train_idx, value=1, dim=0)\n",
    "val_mask = torch.index_fill(torch.zeros_like(y), index=val_idx, value=1, dim=0)\n",
    "test_mask = torch.index_fill(torch.zeros_like(y), index=test_idx, value=1, dim=0)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "del mat\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model have 545920 paramerters in total\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Input \u001B[1;32mIn [53]\u001B[0m, in \u001B[0;36m<cell line: 98>\u001B[1;34m()\u001B[0m\n\u001B[0;32m     96\u001B[0m kmeans \u001B[38;5;241m=\u001B[39m KMeans(n_clusters\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m3\u001B[39m, n_init\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m20\u001B[39m)\n\u001B[0;32m     98\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 99\u001B[0m     z, _, _, _ \u001B[38;5;241m=\u001B[39m \u001B[43mencoder_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgraph_dict\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    100\u001B[0m _ \u001B[38;5;241m=\u001B[39m kmeans\u001B[38;5;241m.\u001B[39mfit_predict(z\u001B[38;5;241m.\u001B[39mdata\u001B[38;5;241m.\u001B[39mcpu()\u001B[38;5;241m.\u001B[39mnumpy())\n\u001B[0;32m    101\u001B[0m encoder_model\u001B[38;5;241m.\u001B[39mcluster_layer\u001B[38;5;241m.\u001B[39mdata \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mtensor(kmeans\u001B[38;5;241m.\u001B[39mcluster_centers_)\u001B[38;5;241m.\u001B[39mto(device)\n",
      "File \u001B[1;32mD:\\Python\\anaconda3\\envs\\black\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *input, **kwargs)\u001B[0m\n\u001B[0;32m   1126\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1127\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1128\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1129\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1130\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1131\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[0;32m   1132\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "Input \u001B[1;32mIn [52]\u001B[0m, in \u001B[0;36mEncoder.forward\u001B[1;34m(self, x, graph_dict, edge_weight)\u001B[0m\n\u001B[0;32m     78\u001B[0m     q \u001B[38;5;241m=\u001B[39m (q\u001B[38;5;241m.\u001B[39mt() \u001B[38;5;241m/\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(q, \u001B[38;5;241m1\u001B[39m))\u001B[38;5;241m.\u001B[39mt()\n\u001B[0;32m     80\u001B[0m     qs\u001B[38;5;241m.\u001B[39mappend(q)\n\u001B[1;32m---> 81\u001B[0m z \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39msum(\u001B[43mtensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mzs\u001B[49m\u001B[43m)\u001B[49m, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     82\u001B[0m \u001B[38;5;28mprint\u001B[39m(z\u001B[38;5;241m.\u001B[39msize())\n\u001B[0;32m     83\u001B[0m z \u001B[38;5;241m=\u001B[39m z \u001B[38;5;241m/\u001B[39m (torch\u001B[38;5;241m.\u001B[39mmax(torch\u001B[38;5;241m.\u001B[39mnorm(z, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m, keepdim\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mepsilon))\n",
      "\u001B[1;31mValueError\u001B[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "def cluster_acc(y_true, y_pred):\n",
    "    y_true = y_true - np.min(y_true)\n",
    "\n",
    "    l1 = list(set(y_true))\n",
    "    numclass1 = len(l1)\n",
    "\n",
    "    l2 = list(set(y_pred))\n",
    "    numclass2 = len(l2)\n",
    "\n",
    "    ind = 0\n",
    "    if numclass1 != numclass2:\n",
    "        for i in l1:\n",
    "            if i in l2:\n",
    "                pass\n",
    "            else:\n",
    "                y_pred[ind] = i\n",
    "                ind += 1\n",
    "\n",
    "    l2 = list(set(y_pred))\n",
    "    numclass2 = len(l2)\n",
    "\n",
    "    if numclass1 != numclass2:\n",
    "        print('error')\n",
    "        return\n",
    "\n",
    "    cost = np.zeros((numclass1, numclass2), dtype=int)\n",
    "    for i, c1 in enumerate(l1):\n",
    "        mps = [i1 for i1, e1 in enumerate(y_true) if e1 == c1]\n",
    "        for j, c2 in enumerate(l2):\n",
    "            mps_d = [i1 for i1 in mps if y_pred[i1] == c2]\n",
    "            cost[i][j] = len(mps_d)\n",
    "\n",
    "    # match two clustering results by Munkres algorithm\n",
    "    m = Munkres()\n",
    "    cost = cost.__neg__().tolist()\n",
    "    indexes = m.compute(cost)\n",
    "\n",
    "    # get the match results\n",
    "    new_predict = np.zeros(len(y_pred))\n",
    "    for i, c in enumerate(l1):\n",
    "        # correponding label in l2:\n",
    "        c2 = l2[indexes[i][1]]\n",
    "\n",
    "        # ai is the index with label==c2 in the pred_label list\n",
    "        ai = [ind for ind, elm in enumerate(y_pred) if elm == c2]\n",
    "        new_predict[ai] = c\n",
    "\n",
    "    acc = metrics.accuracy_score(y_true, new_predict)\n",
    "    f1_macro = metrics.f1_score(y_true, new_predict, average='macro')\n",
    "    precision_macro = metrics.precision_score(y_true, new_predict, average='macro')\n",
    "    recall_macro = metrics.recall_score(y_true, new_predict, average='macro')\n",
    "    f1_micro = metrics.f1_score(y_true, new_predict, average='micro')\n",
    "    precision_micro = metrics.precision_score(y_true, new_predict, average='micro')\n",
    "    recall_micro = metrics.recall_score(y_true, new_predict, average='micro')\n",
    "    return acc, f1_macro\n",
    "\n",
    "def train(encoder_model, contrast_model, optimizer, x, graph_dict, p):\n",
    "    encoder_model.train()\n",
    "    optimizer.zero_grad()\n",
    "    _, z1s, z2s, qs = encoder_model(x, graph_dict)\n",
    "    con_loss = 0\n",
    "    for i in range(len(z1s)):\n",
    "        con_loss += contrast_model(z1s[i], z2s[i])\n",
    "    kl_loss = 0\n",
    "    for i in range(len(qs)):\n",
    "        kl_loss += F.kl_div(qs[i].log(), p, reduction='batchmean')\n",
    "\n",
    "    # con_loss = 0.01 * con_loss\n",
    "    loss = (0.1 * con_loss + 10 * kl_loss) / len(z1s)\n",
    "\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return loss.item(), con_loss.item(), kl_loss.item()\n",
    "\n",
    "\n",
    "aug1 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.5)])\n",
    "aug2 = A.Compose([A.EdgeRemoving(pe=0.5), A.FeatureMasking(pf=0.5)])\n",
    "\n",
    "encoder_model = Encoder(graph_dict=graph_dict, augmentor=(aug1, aug2), hidden_dim=128, num_heads=1, n_clusters=3).to(device)\n",
    "print(\"model have {} paramerters in total\".format(sum(x.numel() for x in encoder_model.parameters())))\n",
    "\n",
    "contrast_model = WithinEmbedContrast(loss=L.BarlowTwins()).to(device)\n",
    "\n",
    "# optimizer = Adam(encoder_model.parameters(), lr=5e-4)\n",
    "optimizer = Adam(encoder_model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "losss = []\n",
    "kl_losss = []\n",
    "con_losss = []\n",
    "nmis = []\n",
    "aris = []\n",
    "ratios = []\n",
    "accs = []\n",
    "f1s = []\n",
    "epochs = 300\n",
    "\n",
    "kmeans = KMeans(n_clusters=3, n_init=20)\n",
    "\n",
    "with torch.no_grad():\n",
    "    z, _, _, _ = encoder_model(x, graph_dict)\n",
    "_ = kmeans.fit_predict(z.data.cpu().numpy())\n",
    "encoder_model.cluster_layer.data = torch.tensor(kmeans.cluster_centers_).to(device)\n",
    "\n",
    "with tqdm(total=epochs, desc='(T)') as pbar:\n",
    "    for epoch in range(epochs):\n",
    "        encoder_model.train()\n",
    "        if epoch % 1 == 0:\n",
    "            # update_interval\n",
    "            _, _, _, tmp_q = encoder_model(x, graph_dict)\n",
    "            data = 0\n",
    "            for q in tmp_q:\n",
    "                data += q\n",
    "            data = data / len(tmp_q)\n",
    "            p = target_distribution(data.data)\n",
    "\n",
    "        loss, con_loss, kl_loss = train(encoder_model, contrast_model, optimizer, x, graph_dict, p)\n",
    "        pbar.set_postfix({'loss': loss, 'con_loss': con_loss, 'kl_loss': kl_loss})\n",
    "        pbar.update()\n",
    "        losss.append(loss)\n",
    "        kl_losss.append(kl_loss)\n",
    "        con_losss.append(con_loss)\n",
    "        ratios.append((100 * kl_loss) / loss)\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            with torch.no_grad():\n",
    "                encoder_model.eval()\n",
    "                z, _, _, _ = encoder_model(x, graph_dict)\n",
    "            pred = kmeans.fit_predict(z.cpu())\n",
    "            nmi = normalized_mutual_info_score(pred, y.cpu())\n",
    "            ari = adjusted_rand_score(pred, y.cpu())\n",
    "            nmis.append((epoch, nmi))\n",
    "            aris.append((epoch, ari))\n",
    "\n",
    "            acc, f1 = cluster_acc(y.cpu().numpy(), pred)\n",
    "\n",
    "            accs.append((epoch, acc))\n",
    "            f1s.append((epoch, f1))\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=2, ncols=3, figsize=(18, 8))\n",
    "axes[0][0].plot(range(epochs), losss)\n",
    "axes[0][1].plot(range(epochs), kl_losss)\n",
    "axes[0][2].plot(range(epochs), con_losss)\n",
    "axes[1][1].plot([i[0] for i in nmis], [i[1] for i in nmis])\n",
    "axes[1][1].plot([i[0] for i in aris], [i[1] for i in aris])\n",
    "axes[1][2].plot([i[0] for i in aris], [i[1] for i in accs])\n",
    "axes[1][2].plot([i[0] for i in aris], [i[1] for i in f1s])\n",
    "plt.show()\n",
    "print('[MAX]NMI: ', max([i[1] for i in nmis]))\n",
    "print('[MAX]ARI: ', max([i[1] for i in aris]))\n",
    "print('[MAX]ACC: ', max([i[1] for i in accs]))\n",
    "print('[MAX]F1: ', max([i[1] for i in f1s]))\n",
    "# test_result = test(encoder_model, data)\n",
    "# print(f'(E): Best test F1Mi={test_result[\"micro_f1\"]:.4f}, F1Ma={test_result[\"macro_f1\"]:.4f}')\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    encoder_model.eval()\n",
    "    z, _, _, qs = encoder_model(x, graph_dict)\n",
    "\n",
    "pred = kmeans.fit_predict(z.cpu())\n",
    "\n",
    "nmi = normalized_mutual_info_score(pred, y.cpu())\n",
    "print('[INFO]NMI: ', nmi)\n",
    "\n",
    "ari = adjusted_rand_score(pred, y.cpu())\n",
    "print('[INFO]ARI: ', ari)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "pred = kmeans.fit_predict(x.cpu())\n",
    "\n",
    "nmi = normalized_mutual_info_score(pred, y.cpu())\n",
    "print('[INFO]NMI: ', nmi)\n",
    "\n",
    "ari = adjusted_rand_score(pred, y.cpu())\n",
    "print('[INFO]ARI: ', ari)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    encoder_model.eval()\n",
    "    z, _, _, _ = encoder_model(x, graph_dict)\n",
    "\n",
    "pred = kmeans.fit_predict(z.cpu())\n",
    "acc, f1 = cluster_acc(y.cpu().numpy(), pred)\n",
    "print(acc, f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
